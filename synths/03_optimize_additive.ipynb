{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing a Harmonic Synthesizer\n",
    "\n",
    "In this section we look at using gradient descent to learn parameters for a harmonic\n",
    "synthesizer to match an instrumental sound.\n",
    "\n",
    "We build on the harmonic synthesizer from the previous section and add several features\n",
    "that support gradient-based optimization. These additions are taken directly from Engel et al.'s\n",
    "differentiable harmonic synthesizer [cite] and include: \n",
    "1) constraining harmonic amplitudes to\n",
    "sum to one; \n",
    "2) adding a global amplitude parameter; \n",
    "3) parameter scaling to constrain the possible range of amplitudes;\n",
    "4) removing frequencies above the Nyquist frequency which will result in aliasing;\n",
    "5) interpolation of parameters from frame rate to sample rate.\n",
    "\n",
    "The updated formula for our harmonic synthesizer is:\n",
    "\n",
    "$$\n",
    "    y[n] = A[n]\\sum_{k=1}^{K}\\hat{\\alpha}_k[n]\\sin\\left(k\\sum_{m=0}^{n}\\omega_{0}[m]\\right)\n",
    "$$\n",
    "\n",
    "where $A[n]$ is a global amplitude parameter, and $\\hat{\\alpha}_k[n]$ is the normalized\n",
    "amplitude for the $k^{\\text{th}}$ sinusoidal component. $\\hat{\\alpha}_k[n]$ is normalized\n",
    "such that $\\sum_{k}\\hat{\\alpha}_k[n] = 1$ and $\\hat{\\alpha}_k[n] > 0$. $\\omega_{0}[n]$ is\n",
    "a time-varying fundamental frequency that is pre-computed using a pitch extraction algorithm.\n",
    "Methods for parameter scaling and removing frequencies above the Nyquist frequency will be introduced inline below.\n",
    "\n",
    "Instead of specifying parameters at a resolution equivalent to the audio sampling rate we'll specify parameters at a frame rate of 100Hz.\n",
    "This sets a reasonable upper bound on the frequency of change of our control signals and has\n",
    "the added benefit of decreasing the dimenionsality of the optimization problem. We only\n",
    "need to learn \\~200 values per harmonic for a second of audio at 16kHz instead of 16k!\n",
    "\n",
    "Finally, we'll use gradient descent with a spectral loss function to match sounds from \n",
    "the [NSynth test dataset](https://magenta.tensorflow.org/datasets/nsynth#files) [cite]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from tqdm import trange\n",
    "\n",
    "import crepe\n",
    "import auraloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_function(\n",
    "    x: torch.Tensor,\n",
    "    exponent: float = 10.0,\n",
    "    max_value: float = 2.0,\n",
    "    threshold: float = 1e-7,\n",
    "):\n",
    "    \"\"\"\n",
    "    Scales a parameter to a range of [threshold, max_value] with a slope of exponent.\n",
    "    A threshold is used to stabilize the gradient near zero.\n",
    "    \"\"\"\n",
    "    return max_value * torch.sigmoid(x) ** math.log(exponent) + threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_above_nyquist(harmonic_amps, frequencies):\n",
    "    harmonic_amps = harmonic_amps * (frequencies < torch.pi).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_harmonic_frequencies(f0, num_harmonics):\n",
    "    # Create integer harmonic ratios and reshape to (1, n_harmonics, 1) so we can\n",
    "    # multiply with fundamental frequency tensor repeated for num_harmonics\n",
    "    harmonic_ratios = torch.arange(1, num_harmonics + 1).view(1, -1, 1)\n",
    "\n",
    "    # Duplicate the fundamental frequency for each harmonic\n",
    "    frequency = f0.unsqueeze(1).repeat(1, num_harmonics, 1)\n",
    "\n",
    "    # Multiply the fundamental frequency by the harmonic ratios\n",
    "    frequency = frequency * harmonic_ratios\n",
    "\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additive_synth(\n",
    "    frequencies: torch.Tensor,  # Angular frequencies (rad / sample) - frame rate\n",
    "    amplitudes: torch.Tensor,  # Amplitudes\n",
    "    n_samples: int,  # Number of samples to synthesize\n",
    "):\n",
    "    assert (\n",
    "        frequencies.ndim == 3\n",
    "    ), \"Frequencies must be 3D (batch, n_frequencies, n_frames)\"\n",
    "    assert (\n",
    "        frequencies.shape == amplitudes.shape\n",
    "    ), \"Frequency and amplitude shapes must match\"\n",
    "\n",
    "    # Upsample frequency and amplitude envelopes to sample rate\n",
    "    f_up = torch.nn.functional.interpolate(frequencies, size=n_samples, mode=\"linear\")\n",
    "    a_up = torch.nn.functional.interpolate(amplitudes, size=n_samples, mode=\"linear\")\n",
    "\n",
    "    # Set initial phase to zero, prepend to frequency envelope\n",
    "    initial_phase = torch.zeros_like(f_up[:, :, :1])\n",
    "    f_up = torch.cat([initial_phase, f_up], dim=-1)[..., :-1]\n",
    "\n",
    "    # Create the phase track and remove the last sample (since we added initial phase)\n",
    "    phase = torch.cumsum(f_up, dim=-1)\n",
    "\n",
    "    y = torch.sin(phase) * a_up\n",
    "    y = torch.sum(y, dim=1)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_synth(\n",
    "    f0: torch.Tensor,  # Angular fundamental frequency (batch, n_samples)\n",
    "    harmonic_amps: torch.Tensor,  # Amplitudes of harmonics (batch, n_harmonics, n_samples)\n",
    "    num_samples: int,  # Number of samples to synthesize\n",
    "    global_amp: torch.Tensor = None,  # Global amplitude, applied to all partials\n",
    "    normalize: bool = True,\n",
    "):\n",
    "    assert f0.ndim == 2, \"Fundamental frequency must be 2D (batch, n_samples)\"\n",
    "    assert (\n",
    "        harmonic_amps.ndim == 3\n",
    "    ), \"Harmonic amplitudes must be 3D (batch, n_harmonics, n_samples)\"\n",
    "\n",
    "    # Get the harmonic frequencies\n",
    "    frequency = get_harmonic_frequencies(f0, harmonic_amps.shape[1])\n",
    "\n",
    "    # Scale the amplitudes\n",
    "    harmonic_amps = scale_function(harmonic_amps)\n",
    "\n",
    "    # Remove frequencies above Nyquist\n",
    "    harmonic_amps = harmonic_amps * (frequency < torch.pi).float()\n",
    "\n",
    "    # Normalize amplitudes to sum to 1 at each sample\n",
    "    if normalize:\n",
    "        harmonic_amps = harmonic_amps / torch.sum(harmonic_amps, dim=1, keepdim=True)\n",
    "\n",
    "    # If no global amplitude is provided, apply a static amplitude of 1\n",
    "    if global_amp is not None:\n",
    "        global_amp = scale_function(global_amp)\n",
    "        harmonic_amps = harmonic_amps * global_amp.unsqueeze(1)\n",
    "\n",
    "    return additive_synth(frequency, harmonic_amps, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio\n",
    "audio, sample_rate = torchaudio.load(\"../audio/reed_acoustic_011-045-050.wav\")\n",
    "\n",
    "# Extract the first 3.25 seconds of the audio (chop silence from the end)\n",
    "audio = audio[:, : int(sample_rate * 2.0)]\n",
    "\n",
    "ipd.Audio(audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_length = int(sample_rate / 200.0)\n",
    "print(f\"hop_length: {hop_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_rate = 100 # Hz\n",
    "# step_size = 1000.0 / frame_rate # ms\n",
    "\n",
    "# _, f0, _, _ = pesto.predict(audio, sample_rate, step_size=step_size, convert_to_freq=True)\n",
    "\n",
    "# plt.plot(f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_rate = 100 # Hz\n",
    "\n",
    "# hop_length = int(sample_rate / frame_rate) # milliseconds\n",
    "\n",
    "# print(hop_length)\n",
    "# print(sample_rate)\n",
    "# f0 = torchcrepe.predict(audio, sample_rate, hop_length=hop_length, batch_size=128, device=\"cpu\", model = 'full')\n",
    "\n",
    "# plt.plot(f0[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rate = 100  # Hz\n",
    "# f0 = torchaudio.functional.detect_pitch_frequency(audio, sample_rate, frame_time=1.0 / frame_rate, win_length=30)\n",
    "\n",
    "# hop_length = int(sample_rate / frame_rate)\n",
    "# f0, _, _ = librosa.pyin(audio.numpy()[0], fmin=50, fmax=2000, sr=sample_rate, hop_length=hop_length)\n",
    "# f0 = torch.from_numpy(f0).unsqueeze(0)\n",
    "# f0 = torch.nan_to_num(f0, nan=0.0)\n",
    "\n",
    "time, frequency, confidence, activation = crepe.predict(\n",
    "    audio.numpy()[0], sample_rate, step_size=1000 / frame_rate, viterbi=True\n",
    ")\n",
    "f0 = torch.from_numpy(frequency).unsqueeze(0)\n",
    "\n",
    "# timesteps, pitch, confidence, activations = pesto.predict(audio, sample_rate, step_size=1000.0/frame_rate)\n",
    "# f0 = pitch.unsqueeze(0)\n",
    "\n",
    "plt.plot(f0[0].numpy())\n",
    "print(f0.min(), f0.max())\n",
    "\n",
    "print(f0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.stft(\n",
    "    audio,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    return_complex=True,\n",
    "    window=torch.hann_window(2048),\n",
    ")\n",
    "X_mag = torch.abs(X)\n",
    "X_db = 20.0 * torch.log10(X_mag + 1e-6)\n",
    "\n",
    "plt.imshow(X_db[0].numpy(), aspect=\"auto\", origin=\"lower\")\n",
    "\n",
    "f0_bins = f0 * 2048 / sample_rate\n",
    "f0_bins = f0_bins.unsqueeze(0)\n",
    "f0_bins = torch.nn.functional.interpolate(f0_bins, size=X_db.shape[2], mode=\"linear\")\n",
    "# plt.plot(f0_bins[0,0].numpy(), 'r')\n",
    "# plt.plot(f0_bins[0,0].numpy() * 4.0, 'r')\n",
    "\n",
    "# TODO - fix the scaling on this\n",
    "y_ticks = plt.yticks()\n",
    "y_tick = torch.logspace(5, 13, 6, base=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additive_synth(\n",
    "    frequencies: torch.Tensor,  # Angular frequencies (rad / sample) - frame rate\n",
    "    amplitudes: torch.Tensor,  # Amplitudes\n",
    "    n_samples: int,  # Number of samples to synthesize\n",
    "):\n",
    "    assert (\n",
    "        frequencies.ndim == 3\n",
    "    ), \"Frequencies must be 3D (batch, n_frequencies, n_frames)\"\n",
    "    assert (\n",
    "        frequencies.shape == amplitudes.shape\n",
    "    ), \"Frequency and amplitude shapes must match\"\n",
    "\n",
    "    # Upsample frequency and amplitude envelopes to sample rate\n",
    "    f_up = torch.nn.functional.interpolate(frequencies, size=n_samples, mode=\"linear\")\n",
    "    a_up = torch.nn.functional.interpolate(amplitudes, size=n_samples, mode=\"linear\")\n",
    "\n",
    "    # Set initial phase to zero, prepend to frequency envelope\n",
    "    initial_phase = torch.zeros_like(f_up[:, :, :1])\n",
    "    f_up = torch.cat([initial_phase, f_up], dim=-1)[..., :-1]\n",
    "\n",
    "    # Create the phase track and remove the last sample (since we added initial phase)\n",
    "    phase = torch.cumsum(f_up, dim=-1)\n",
    "\n",
    "    y = torch.sin(phase) * a_up\n",
    "    y = torch.sum(y, dim=1)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_function(x):\n",
    "    return 2.0 * torch.sigmoid(x) ** math.log(10.0) + 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_harmonic_frequencies(f0, num_harmonics):\n",
    "    # Create integer harmonic ratios and reshape to (1, n_harmonics, 1) so we can\n",
    "    # multiply with fundamental frequency tensor repeated for num_harmonics\n",
    "    harmonic_ratios = torch.arange(1, num_harmonics + 1).view(1, -1, 1)\n",
    "\n",
    "    # Duplicate the fundamental frequency for each harmonic\n",
    "    frequency = f0.unsqueeze(1).repeat(1, num_harmonics, 1)\n",
    "\n",
    "    # Multiply the fundamental frequency by the harmonic ratios\n",
    "    frequency = frequency * harmonic_ratios\n",
    "\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_synth(\n",
    "    f0: torch.Tensor,  # Angular fundamental frequency (batch, n_samples)\n",
    "    harmonic_amps: torch.Tensor,  # Amplitudes of harmonics (batch, n_harmonics, n_samples)\n",
    "    num_samples: int,  # Number of samples to synthesize\n",
    "    global_amp: torch.Tensor = None,  # Global amplitude, applied to all partials\n",
    "    normalize: bool = True,\n",
    "):\n",
    "    assert f0.ndim == 2, \"Fundamental frequency must be 2D (batch, n_samples)\"\n",
    "    assert (\n",
    "        harmonic_amps.ndim == 3\n",
    "    ), \"Harmonic amplitudes must be 3D (batch, n_harmonics, n_samples)\"\n",
    "\n",
    "    # Get the harmonic frequencies\n",
    "    frequency = get_harmonic_frequencies(f0, harmonic_amps.shape[1])\n",
    "\n",
    "    # Scale the amplitudes\n",
    "    harmonic_amps = scale_function(harmonic_amps)\n",
    "\n",
    "    # Remove frequencies above Nyquist\n",
    "    harmonic_amps = harmonic_amps * (frequency < torch.pi).float()\n",
    "\n",
    "    # Normalize amplitudes to sum to 1 at each sample\n",
    "    if normalize:\n",
    "        harmonic_amps = harmonic_amps / torch.sum(harmonic_amps, dim=1, keepdim=True)\n",
    "\n",
    "    # If no global amplitude is provided, apply a static amplitude of 1\n",
    "    if global_amp is not None:\n",
    "        global_amp = scale_function(global_amp)\n",
    "        harmonic_amps = harmonic_amps * global_amp.unsqueeze(1)\n",
    "\n",
    "    return additive_synth(frequency, harmonic_amps, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to angular frequency\n",
    "w0 = f0 * 2 * torch.pi / sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_harmonics = 80\n",
    "\n",
    "amplitudes = 1.0 / (torch.arange(num_harmonics) + 1).float()\n",
    "print(amplitudes)\n",
    "amplitudes = torch.ones(1, num_harmonics, w0.shape[-1]) * amplitudes.view(1, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w0.shape, amplitudes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = harmonic_synth(w0, amplitudes, audio.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(y[0].numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 512\n",
    "X = torch.stft(\n",
    "    y, n_fft=N, hop_length=N // 4, return_complex=True, window=torch.hann_window(N)\n",
    ")\n",
    "X_mag = torch.abs(X)\n",
    "X_db = 20.0 * torch.log10(X_mag + 1e-6)\n",
    "\n",
    "plt.imshow(X_db[0].numpy(), aspect=\"auto\", origin=\"lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ffts = [2048, 1024, 512, 256, 128, 64]\n",
    "# n_ffts = [128]\n",
    "hop_sizes = [n // 4 for n in n_ffts]\n",
    "loss_fn = auraloss.freq.MultiResolutionSTFTLoss(\n",
    "    fft_sizes=n_ffts,\n",
    "    hop_sizes=hop_sizes,\n",
    "    win_lengths=n_ffts,\n",
    "    w_sc=0.0,\n",
    "    w_lin_mag=1.0,\n",
    "    w_log_mag=1.0,\n",
    ")\n",
    "# loss_fn = auraloss.freq.MultiResolutionSTFTLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: zero-out frequencies above Nyquist\n",
    "amp_param = torch.randn_like(amplitudes)\n",
    "frequencies = get_harmonic_frequencies(w0, amp_param.shape[1])\n",
    "# amp_param = amp_param * (frequencies < torch.pi).float()\n",
    "\n",
    "amp_param = torch.nn.Parameter(amp_param)\n",
    "global_amp = torch.nn.Parameter(torch.rand_like(w0))\n",
    "\n",
    "optimizer = torch.optim.Adam([amp_param, global_amp], lr=0.05)\n",
    "# optimizer = torch.optim.SGD([amp_param, global_amp], lr=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    scale_function(amp_param)[0].detach().numpy(),\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log = []\n",
    "t = trange(1000, desc=\"Error\", leave=True)\n",
    "for i in t:\n",
    "    # 1. Compute a forward pass using our learned parameter\n",
    "    y_pred = harmonic_synth(w0, amp_param, audio.shape[-1], global_amp=global_amp)\n",
    "\n",
    "    # 2. Compute multiresolution spectral resolution loss\n",
    "    loss = loss_fn(\n",
    "        audio.unsqueeze(0), y_pred.unsqueeze(0)\n",
    "    )  # + 0.1 * torch.mean(amp_param)\n",
    "\n",
    "    # Store the current loss value for plotting later\n",
    "    loss_log.append(loss.item())\n",
    "\n",
    "    # 3. Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Compute the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    t.set_description(f\"Error: {loss.detach().cpu().numpy()}\")\n",
    "    t.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(torch.abs(amp_param * (frequencies > torch.pi).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(y_pred[0].detach().numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.stft(\n",
    "    y_pred.detach(),\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    return_complex=True,\n",
    "    window=torch.hann_window(2048),\n",
    ")\n",
    "X_mag = torch.abs(X)\n",
    "X_db = 20.0 * torch.log10(X_mag)\n",
    "\n",
    "plt.imshow(X_db[0].numpy(), aspect=\"auto\", origin=\"lower\")\n",
    "\n",
    "\n",
    "# TODO - fix the scaling on this\n",
    "y_ticks = plt.yticks()\n",
    "y_tick = torch.logspace(5, 13, 6, base=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scale_function(global_amp[0]).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    scale_function(amp_param)[0].detach().numpy(),\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_amp = scale_function(amp_param)\n",
    "print(scaled_amp.shape)\n",
    "print(scaled_amp.min(), scaled_amp.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_function(torch.tensor(-20.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ismir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
