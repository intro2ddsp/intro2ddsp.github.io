{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing a Harmonic Synthesizer\n",
    "\n",
    "In this section we look at using gradient descent to learn parameters for a harmonic\n",
    "synthesizer to match an instrumental sound.\n",
    "\n",
    "We build on the harmonic synthesizer from the previous section and add several features\n",
    "that support gradient-based optimization. These additions are taken directly from Engel et al.'s\n",
    "differentiable harmonic synthesizer [cite] and include: \n",
    "1) constraining harmonic amplitudes to\n",
    "sum to one; \n",
    "2) adding a global amplitude parameter; \n",
    "3) parameter scaling to constrain the possible range of amplitudes;\n",
    "4) removing frequencies above the Nyquist frequency which will result in aliasing;\n",
    "5) interpolation of parameters from frame rate to sample rate.\n",
    "\n",
    "The updated formula for our harmonic synthesizer is:\n",
    "\n",
    "$$\n",
    "    y[n] = A[n]\\sum_{k=1}^{K}\\hat{\\alpha}_k[n]\\sin\\left(k\\sum_{m=0}^{n}\\omega_{0}[m]\\right)\n",
    "$$\n",
    "\n",
    "where $A[n]$ is a global amplitude parameter, and $\\hat{\\alpha}_k[n]$ is the normalized\n",
    "amplitude for the $k^{\\text{th}}$ sinusoidal component. $\\hat{\\alpha}_k[n]$ is normalized\n",
    "such that $\\sum_{k}\\hat{\\alpha}_k[n] = 1$ and $\\hat{\\alpha}_k[n] > 0$. $\\omega_{0}[n]$ is\n",
    "a time-varying fundamental frequency that is pre-computed using a pitch extraction algorithm.\n",
    "Methods for parameter scaling and removing frequencies above the Nyquist frequency will be introduced inline below.\n",
    "\n",
    "Instead of specifying parameters at a resolution equivalent to the audio sampling rate we'll specify parameters at a frame rate of 100Hz.\n",
    "This sets a reasonable upper bound on the frequency of change of our control signals and has\n",
    "the added benefit of decreasing the dimenionsality of the optimization problem. We only\n",
    "need to learn \\~200 values per harmonic for a second of audio at 16kHz instead of 16k!\n",
    "\n",
    "Finally, we'll use gradient descent with a spectral loss function to match sounds from \n",
    "the [NSynth test dataset](https://magenta.tensorflow.org/datasets/nsynth#files) [cite]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mplticker\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import IPython.display as ipd\n",
    "from tqdm import trange\n",
    "\n",
    "import crepe\n",
    "import auraloss\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_function(\n",
    "    x: torch.Tensor,\n",
    "    exponent: float = 10.0,\n",
    "    max_value: float = 2.0,\n",
    "    threshold: float = 1e-7,\n",
    "):\n",
    "    \"\"\"\n",
    "    Scales a parameter to a range of [threshold, max_value] with a slope of exponent.\n",
    "    A threshold is used to stabilize the gradient near zero.\n",
    "    \"\"\"\n",
    "    return max_value * torch.sigmoid(x) ** math.log(exponent) + threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_above_nyquist(\n",
    "    harmonic_amps: torch.Tensor,\n",
    "    frequencies: torch.Tensor,\n",
    "):\n",
    "    return harmonic_amps * (frequencies < torch.pi).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_harmonic_frequencies(f0, num_harmonics):\n",
    "    # Create integer harmonic ratios and reshape to (1, n_harmonics, 1) so we can\n",
    "    # multiply with fundamental frequency tensor repeated for num_harmonics\n",
    "    harmonic_ratios = torch.arange(1, num_harmonics + 1).view(1, -1, 1)\n",
    "\n",
    "    # Duplicate the fundamental frequency for each harmonic\n",
    "    frequency = f0.unsqueeze(1).repeat(1, num_harmonics, 1)\n",
    "\n",
    "    # Multiply the fundamental frequency by the harmonic ratios\n",
    "    frequency = frequency * harmonic_ratios\n",
    "\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additive_synth(\n",
    "    frequencies: torch.Tensor,  # Angular frequencies (rad / sample) - frame rate\n",
    "    amplitudes: torch.Tensor,  # Amplitudes\n",
    "    n_samples: int,  # Number of samples to synthesize\n",
    "):\n",
    "    assert (\n",
    "        frequencies.ndim == 3\n",
    "    ), \"Frequencies must be 3D (batch, n_frequencies, n_frames)\"\n",
    "    assert (\n",
    "        frequencies.shape == amplitudes.shape\n",
    "    ), \"Frequency and amplitude shapes must match\"\n",
    "\n",
    "    # Upsample frequency and amplitude envelopes to sample rate\n",
    "    f_up = torch.nn.functional.interpolate(frequencies, size=n_samples, mode=\"linear\")\n",
    "    a_up = torch.nn.functional.interpolate(amplitudes, size=n_samples, mode=\"linear\")\n",
    "\n",
    "    # Set initial phase to zero, prepend to frequency envelope\n",
    "    initial_phase = torch.zeros_like(f_up[:, :, :1])\n",
    "    f_up = torch.cat([initial_phase, f_up], dim=-1)[..., :-1]\n",
    "\n",
    "    # Create the phase track and remove the last sample (since we added initial phase)\n",
    "    phase = torch.cumsum(f_up, dim=-1)\n",
    "\n",
    "    y = torch.sin(phase) * a_up\n",
    "    y = torch.sum(y, dim=1)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_synth(\n",
    "    f0: torch.Tensor,  # Angular fundamental frequency (batch, n_samples)\n",
    "    global_amp: torch.Tensor,  # Global amplitude (batch, n_samples)\n",
    "    harmonic_amps: torch.Tensor,  # Amplitudes of harmonics (batch, n_harmonics, n_samples)\n",
    "    num_samples: int,  # Number of samples to synthesize\n",
    "):\n",
    "    assert f0.ndim == 2, \"Fundamental frequency must be 2D (batch, n_samples)\"\n",
    "    assert (\n",
    "        harmonic_amps.ndim == 3\n",
    "    ), \"Harmonic amplitudes must be 3D (batch, n_harmonics, n_samples)\"\n",
    "\n",
    "    # Get the harmonic frequencies\n",
    "    frequency = get_harmonic_frequencies(f0, harmonic_amps.shape[1])\n",
    "\n",
    "    # Scale the amplitudes\n",
    "    harmonic_amps = scale_function(harmonic_amps)\n",
    "    global_amp = scale_function(global_amp)\n",
    "\n",
    "    # Remove frequencies above Nyquist\n",
    "    harmonic_amps = remove_above_nyquist(harmonic_amps, frequency)\n",
    "\n",
    "    # Normalize the harmonic amplitudes\n",
    "    harmonic_amps = harmonic_amps / torch.sum(harmonic_amps, dim=1, keepdim=True)\n",
    "\n",
    "    # Multiply all harmonic amplitudes by the global amplitude\n",
    "    harmonic_amps = harmonic_amps * global_amp.unsqueeze(1)\n",
    "\n",
    "    return additive_synth(frequency, harmonic_amps, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio\n",
    "audio, sample_rate = torchaudio.load(\"../audio/reed_acoustic_011-045-050.wav\")\n",
    "\n",
    "# Extract the first 3.25 seconds of the audio (chop silence from the end)\n",
    "audio = audio[:, : int(sample_rate * 2.0)]\n",
    "\n",
    "ipd.Audio(audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pitch extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rate = 100  # Hz\n",
    "frame_ms = 1000 / frame_rate  # ms\n",
    "\n",
    "_, f0, _, _ = crepe.predict(\n",
    "    audio.numpy()[0], sample_rate, step_size=frame_ms, viterbi=True\n",
    ")\n",
    "f0 = torch.from_numpy(f0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f0[0].numpy())\n",
    "print(f0.min(), f0.max())\n",
    "\n",
    "print(f0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_spectrogram(\n",
    "    x: torch.Tensor, sample_rate: int, fig: plt.Figure = None, ax: plt.Axes = None\n",
    "):\n",
    "    n_fft = 2048\n",
    "    hop_length = 512\n",
    "    X = torch.stft(\n",
    "        x,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        return_complex=True,\n",
    "        window=torch.hann_window(n_fft),\n",
    "    )\n",
    "\n",
    "    # Convert to decibels\n",
    "    X_mag = torch.abs(X)\n",
    "    X_db = 20.0 * torch.log10(X_mag + 1e-6)\n",
    "\n",
    "    # Get frequencies for each FFT bin in hertz\n",
    "    fft_freqs = torch.abs(torch.fft.fftfreq(2048, 1 / sample_rate)[: X_db.shape[1]])\n",
    "\n",
    "    # Time in seconds for each frame\n",
    "    times = torch.arange(X_db.shape[-1]) * hop_length / sample_rate\n",
    "\n",
    "    # Plot the spectrogram\n",
    "    if fig is None and ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    ax.pcolormesh(times, fft_freqs, X_db[0].numpy())\n",
    "\n",
    "    # Set the y-axis to log scale\n",
    "    ax.set_yscale(\"symlog\", base=2.0)\n",
    "    ax.set_ylim(40.0, 8000.0)\n",
    "\n",
    "    ax.yaxis.set_major_formatter(mplticker.ScalarFormatter())\n",
    "    ax.yaxis.set_label_text(\"Frequency (Hz)\")\n",
    "\n",
    "    ax.xaxis.set_major_formatter(mplticker.ScalarFormatter())\n",
    "    ax.xaxis.set_label_text(\"Time (Seconds)\")\n",
    "\n",
    "    return fig, ax, times, fft_freqs\n",
    "\n",
    "\n",
    "fig, ax, xaxis, yaxis = plot_spectrogram(audio, sample_rate)\n",
    "\n",
    "f0_interp = f0.unsqueeze(0)\n",
    "f0_interp = torch.nn.functional.interpolate(\n",
    "    f0_interp, size=xaxis.shape[0], mode=\"linear\"\n",
    ")\n",
    "ax.plot(xaxis, f0_interp[0, 0].numpy(), color=\"red\", label=\"Detected F0\")\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title(\"Target Spectrogram with F0\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiresolution Spectral Loss\n",
    "\n",
    "[todo] write own code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ffts = [2048, 1024, 512, 256, 128, 64]\n",
    "hop_sizes = [n // 4 for n in n_ffts]\n",
    "loss_fn = auraloss.freq.MultiResolutionSTFTLoss(\n",
    "    fft_sizes=n_ffts,\n",
    "    hop_sizes=hop_sizes,\n",
    "    win_lengths=n_ffts,\n",
    "    w_sc=0.0,\n",
    "    w_lin_mag=1.0,\n",
    "    w_log_mag=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fundamental frequency to angular frequency\n",
    "w0 = f0 * 2 * torch.pi / sample_rate\n",
    "\n",
    "# Create a harmonic amplitude envelope\n",
    "num_harmonics = 80\n",
    "harmonic_amplitudes = torch.randn(1, num_harmonics, f0.shape[-1])\n",
    "\n",
    "# Create PyTorch parameters - these are the variables we will optimize\n",
    "harmonic_amplitudes = torch.nn.Parameter(harmonic_amplitudes)\n",
    "global_amp = torch.nn.Parameter(torch.randn_like(w0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([harmonic_amplitudes, global_amp], lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = harmonic_synth(w0, global_amp, harmonic_amplitudes, audio.shape[-1])\n",
    "fig, ax, *_ = plot_spectrogram(y_hat.detach(), sample_rate)\n",
    "\n",
    "ax.set_title(\"Initial Randomized Harmonic Synthesis\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log = []\n",
    "audio_log = []\n",
    "t = trange(1000, desc=\"Error\", leave=True)\n",
    "for i in t:\n",
    "    # 1. Compute a forward pass using our learned parameter\n",
    "    y_pred = harmonic_synth(w0, global_amp, harmonic_amplitudes, audio.shape[-1])\n",
    "\n",
    "    # 2. Compute multiresolution spectral resolution loss\n",
    "    loss = loss_fn(audio.unsqueeze(0), y_pred.unsqueeze(0))\n",
    "\n",
    "    # Store the current loss value for plotting later\n",
    "    loss_log.append(loss.item())\n",
    "\n",
    "    # 3. Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Compute the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log audio and loss\n",
    "    if i % 50 == 0:\n",
    "        audio_log.append(y_pred.detach().cpu().numpy())\n",
    "\n",
    "    t.set_description(f\"Error: {loss.detach().cpu().numpy()}\")\n",
    "    t.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "fig, im_ax, *_ = plot_spectrogram(y_hat.detach(), sample_rate, fig=fig, ax=axes[0])\n",
    "\n",
    "axes[1].set_ylim(0.0, max(loss_log))\n",
    "axes[1].set_xlim(0, len(loss_log))\n",
    "\n",
    "(line,) = axes[1].plot([], [], lw=2)\n",
    "\n",
    "num_frames = len(audio_log)\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    iteration = i * 50\n",
    "    im_ax.set_title(f\"Harmonic Synthesis Iteration: {iteration}\")\n",
    "    _ = plot_spectrogram(torch.from_numpy(audio_log[i]), sample_rate, fig=fig, ax=im_ax)\n",
    "    axes[1].set_title(\"Loss: {:.4f}\".format(loss_log[iteration]))\n",
    "    line.set_data(torch.arange(iteration), loss_log[:iteration])\n",
    "    return (line,)\n",
    "\n",
    "\n",
    "# Create the animation\n",
    "anim = FuncAnimation(fig, animate, frames=len(audio_log), interval=250, blit=True)\n",
    "\n",
    "plt.close(fig)\n",
    "# To display the animation in the Jupyter notebook:\n",
    "display(ipd.HTML(anim.to_html5_video()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(y_pred[0].detach().numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "_ = plot_spectrogram(y_pred.detach(), sample_rate, fig, axes[0])\n",
    "_ = plot_spectrogram(audio, sample_rate, fig, axes[1])\n",
    "\n",
    "axes[0].set_title(\"Optimized Harmonic Synthesis\")\n",
    "axes[1].set_title(\"Target Audio\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ismir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
