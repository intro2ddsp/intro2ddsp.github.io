@article{arik2018fast,
	title        = {Fast spectrogram inversion using multi-head convolutional neural networks},
	author       = {Ar{\i}k, Sercan {\"O} and Jun, Heewoo and Diamos, Gregory},
	year         = 2018,
	journal      = {IEEE Signal Processing Letters},
	publisher    = {IEEE},
	volume       = 26,
	number       = 1,
	pages        = {94--98}
}
@article{barahona2023noisebandnet,
	title        = {NoiseBandNet: Controllable Time-Varying Neural Synthesis of Sound Effects Using Filterbanks},
	author       = {Barahona-R{\'\i}os, Adri{\'a}n and Collins, Tom},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2307.08007}
}
@inproceedings{cherep2023synthax,
	title        = {SynthAX: A Fast Modular Synthesizer in JAX},
	author       = {Cherep, Manuel and Singh, Nikhil},
	year         = 2023,
	month        = {May},
	booktitle    = {Audio Engineering Society Convention 155},
	url          = {http://www.aes.org/e-lib/browse.cfm?elib=22261}
}
@inproceedings{diaz_rigid-body_2022,
	title        = {Rigid-Body Sound Synthesis with Differentiable Modal Resonators},
	author       = {Diaz, Rodrigo and Hayes, Ben and Saitis, Charalampos and Fazekas, Gy{\"o}rgy and Sandler, Mark},
	year         = 2023,
	booktitle    = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	organization = {IEEE}
}
@inproceedings{engel_ddsp_2020,
	title        = {{{DDSP}}: {{Differentiable Digital Signal Processing}}},
	shorttitle   = {{{DDSP}}},
	author       = {Engel, Jesse and Hantrakul, Lamtharn (Hanoi) and Gu, Chenjie and Roberts, Adam},
	year         = 2020,
	month        = apr,
	booktitle    = {8th {{International Conference}} on {{Learning Representations}}},
	urldate      = {2020-01-29}
}
@inproceedings{engel2017neural,
	title        = {Neural audio synthesis of musical notes with wavenet autoencoders},
	author       = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Norouzi, Mohammad and Eck, Douglas and Simonyan, Karen},
	year         = 2017,
	booktitle    = {International Conference on Machine Learning},
	pages        = {1068--1077},
	organization = {PMLR}
}
@inproceedings{hayes2023sinusoidal,
	title        = {Sinusoidal Frequency Estimation by Gradient Descent},
	author       = {Hayes, Ben and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
	year         = 2023,
	booktitle    = {ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages        = {1--5},
	organization = {IEEE}
}
@inproceedings{kim2018crepe,
	title        = {Crepe: A convolutional representation for pitch estimation},
	author       = {Kim, Jong Wook and Salamon, Justin and Li, Peter and Bello, Juan Pablo},
	year         = 2018,
	booktitle    = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages        = {161--165},
	organization = {IEEE}
}
@article{liu2023ddsp,
	title        = {DDSP-SFX: Acoustically-guided sound effects generation with differentiable digital signal processing},
	author       = {Liu, Yunyi and Jin, Craig and Gunawan, David},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2309.08060}
}
@article{masuda_improving_2023,
	title        = {Improving {{Semi-Supervised Differentiable Synthesizer Sound Matching}} for {{Practical Applications}}},
	author       = {Masuda, Naotake and Saito, Daisuke},
	year         = 2023,
	journal      = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	volume       = 31,
	pages        = {863--875},
	doi          = {10.1109/TASLP.2023.3237161},
	issn         = {2329-9304}
}
@inproceedings{renault_differentiable_2022,
	title        = {Differentiable {{Piano Model}} for {{Midi-to-Audio Performance Synthesis}}},
	author       = {Renault, Lenny and Mignot, R{\'e}mi and Roebel, Axel},
	year         = 2022,
	booktitle    = {Proceedings of the 25th {{International Conference}} on {{Digital Audio Effects}}},
	address      = {{Vienna, Austria}},
	pages        = 8
}
@article{serra_spectral_1990,
	title        = {Spectral {{Modeling Synthesis}}: {{A Sound Analysis}}/{{Synthesis System Based}} on a {{Deterministic Plus Stochastic Decomposition}}},
	shorttitle   = {Spectral {{Modeling Synthesis}}},
	author       = {Serra, Xavier and Smith, Julius},
	year         = 1990,
	journal      = {Computer Music Journal},
	volume       = 14,
	number       = 4,
	pages        = {12--24},
	doi          = {10.2307/3680788},
	issn         = {0148-9267},
	url          = {www.jstor.org/stable/3680788},
	urldate      = {2019-12-21}
}
@inproceedings{shier2023differentiable,
	title        = {Differentiable Modelling of Percussive Audio with Transient and Spectral Synthesis},
	author       = {Shier, Jordie and Caspe, Franco and Robertson, Andrew and Sandler, Mark and Saitis, Charalampos and McPherson, Andrew},
	year         = 2023,
	booktitle    = {Proceedings of the 10th Convention of the European Acoustics Association Forum Acusticum 2023}
}
@inproceedings{turian_one_2021,
	title        = {One {{Billion Audio Sounds}} from {{GPU-enabled Modular Synthesis}}},
	author       = {Turian, Joseph and Shier, Jordie and Tzanetakis, George and McNally, Kirk and Henry, Max},
	year         = 2021,
	booktitle    = {Proceedings of the 23rd International Conference on Digital Audio Effects}
}
@article{wang2019neural,
	title        = {Neural source-filter waveform models for statistical parametric speech synthesis},
	author       = {Wang, Xin and Takaki, Shinji and Yamagishi, Junichi},
	year         = 2019,
	journal      = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	publisher    = {IEEE},
	volume       = 28,
	pages        = {402--415}
}
@inproceedings{wu_ddsp-based_2022,
	title        = {{{DDSP-based Singing Vocoders}}: {{A New Subtractive-based Synthesizer}} and {{A Comprehensive Evaluation}}},
	author       = {Wu, Da-Yi and Hsiao, Wen-Yi and Yang, Fu-Rong and Friedman, Oscar and Jackson, Warren and Bruzenak, Scott and Liu, Yi-Wen and Yang, Yi-Hsuan},
	year         = 2022,
	booktitle    = {Proceedings of the 23rd International Society for Music Information Retrieval Conference},
	pages        = {76--83}
}
@book{dafx,
  title     = {DAFX - Digital Audio Effects},
  author    = {Zöelzer, Udo},
  year      = {2011},
  publisher = {John Wiley \& Sons}
}


@article{golf,
  title={Singing Voice Synthesis Using Differentiable LPC and Glottal-Flow-Inspired Wavetables},
  author={Yu, Chin-Yun and Fazekas, Gy{\"o}rgy},
  journal={arXiv preprint arXiv:2306.17252},
  year={2023}
}

@inproceedings{nercessian2021lightweight,
  title={Lightweight and interpretable neural modeling of an audio distortion effect using hyperconditioned differentiable biquads},
  author={Nercessian, Shahan and Sarroff, Andy and Werner, Kurt James},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={890--894},
  year={2021},
  organization={IEEE}
}

@article{makhoul1975linear,
  title={Linear prediction: A tutorial review},
  author={Makhoul, John},
  journal={Proceedings of the IEEE},
  volume={63},
  number={4},
  pages={561--580},
  year={1975},
  publisher={IEEE}
}

@article{fant1995lf,
  title={The LF-model revisited. Transformations and frequency domain analysis},
  author={Fant, Gunnar},
  journal={Speech Trans. Lab. Q. Rep., Royal Inst. of Tech. Stockholm},
  volume={2},
  number={3},
  pages={40},
  year={1995}
}

@book{smith_filters_2007,
	title        = {Introduction to Digital Filters with Audio Applications},
	author       = {Julius O. Smith},
	year         = 2007,
	publisher    = {W3K Publishing},
	address      = {http://www.w3k.org/books/},
	isbn         = {978-0-9745607-1-7}
}
@inproceedings{wang_neural_2019,
	title        = {Neural {{Harmonic-plus-Noise Waveform Model}} with {{Trainable Maximum Voice Frequency}} for {{Text-to-Speech Synthesis}}},
	author       = {Wang, Xin and Yamagishi, Junichi},
	year         = 2019,
	month        = sep,
	booktitle    = {10th {{ISCA Workshop}} on {{Speech Synthesis}} ({{SSW}} 10)},
	publisher    = {{ISCA}},
	pages        = {1--6},
	doi          = {10.21437/SSW.2019-1},
	urldate      = {2023-07-04},
	abstract     = {Neural source-filter (NSF) models are deep neural networks that produce waveforms given input acoustic features. They use dilated-convolution-based neural filter modules to filter sinebased excitation for waveform generation, which is different from WaveNet and flow-based models. One of the NSF models, called harmonic-plus-noise NSF (h-NSF) model, uses separate pairs of source and neural filters to generate harmonic and noise waveform components. It is close to WaveNet in terms of speech quality while being superior in generation speed.},
	file         = {/Users/benhayes/Zotero/storage/J6NCJG6J/Wang and Yamagishi - 2019 - Neural Harmonic-plus-Noise Waveform Model with Tra.pdf}
}
@article{jorda_performance_2019,
	title        = {Performance Evaluation of cuDNN Convolution Algorithms on NVIDIA Volta GPUs},
	author       = {Jordà, Marc and Valero-Lara, Pedro and Peña, Antonio J.},
	year         = 2019,
	journal      = {IEEE Access},
	volume       = 7,
	number       = {},
	pages        = {70461--70473},
	doi          = {10.1109/ACCESS.2019.2918851}
}
@misc{barahona-rios_noisebandnet_2023,
	title        = {{{NoiseBandNet}}: {{Controllable Time-Varying Neural Synthesis}} of {{Sound Effects Using Filterbanks}}},
	shorttitle   = {{{NoiseBandNet}}},
	author       = {{Barahona-R{\'i}os}, Adri{\'a}n and Collins, Tom},
	year         = 2023,
	month        = jul,
	publisher    = {{arXiv}},
	number       = {arXiv:2307.08007},
	doi          = {10.48550/arXiv.2307.08007},
	primaryclass = {cs, eess}
}
@article{hayes_review_2023,
	title        = {A {{Review}} of {{Differentiable Digital Signal Processing}} for {{Music}} \& {{Speech Synthesis}}},
	author       = {Hayes, Ben and Shier, Jordie and Fazekas, Gy{\"o}rgy and McPherson, Andrew and Saitis, Charalampos},
	year         = 2023,
	month        = aug,
	publisher    = {{arXiv}},
	number       = {arXiv:2308.15422},
	doi          = {10.48550/arXiv.2308.15422},
	urldate      = {2023-09-05},
	eprint       = {2308.15422},
	primaryclass = {cs, eess}
}
@inproceedings{oord_wavenet_2016,
	title        = {WaveNet: A Generative Model for Raw Audio},
	author       = {Aäron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alexander Graves and Nal Kalchbrenner and Andrew Senior and Koray Kavukcuoglu},
	year         = 2016,
	booktitle    = {Arxiv},
	url          = {https://arxiv.org/abs/1609.03499}
}
@inproceedings{oord_parallel_2018,
	title        = {Parallel {W}ave{N}et: Fast High-Fidelity Speech Synthesis},
	author       = {van den Oord, Aaron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and van den Driessche, George and Lockhart, Edward and Cobo, Luis and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
	year         = 2018,
	month        = {10--15 Jul},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 80,
	pages        = {3918--3926},
	url          = {https://proceedings.mlr.press/v80/oord18a.html},
	editor       = {Dy, Jennifer and Krause, Andreas},
	pdf          = {http://proceedings.mlr.press/v80/oord18a/oord18a.pdf},
	abstract     = {The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today’s massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, a 1000x speed up relative to the original WaveNet, and capable of serving multiple English and Japanese voices in a production setting.}
}
@inproceedings{pons_upsampling_2021,
	title        = {Upsampling Artifacts in Neural Audio Synthesis},
	author       = {Pons, Jordi and Pascual, Santiago and Cengarle, Giulio and Serrà, Joan},
	year         = 2021,
	booktitle    = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	volume       = {},
	number       = {},
	pages        = {3005--3009},
	doi          = {10.1109/ICASSP39728.2021.9414913}
}
@inproceedings{engel_gansynth_2019,
	title        = {{GANS}ynth: Adversarial Neural Audio Synthesis},
	author       = {Jesse Engel and Kumar Krishna Agrawal and Shuo Chen and Ishaan Gulrajani and Chris Donahue and Adam Roberts},
	year         = 2019,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=H1xQVn09FX}
}
@inproceedings{kong_hifi-gan_2020,
	title        = {HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis},
	author       = {Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
	year         = 2020,
	booktitle    = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
	location     = {Vancouver, BC, Canada},
	publisher    = {Curran Associates Inc.},
	address      = {Red Hook, NY, USA},
	series       = {NIPS'20},
	isbn         = 9781713829546,
	abstract     = {Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.},
	articleno    = 1428,
	numpages     = 12
}
@inproceedings{juvela_gelp_2019,
	title        = {{GELP: GAN-Excited Linear Prediction for Speech Synthesis from Mel-Spectrogram}},
	author       = {Lauri Juvela and Bajibabu Bollepalli and Junichi Yamagishi and Paavo Alku},
	year         = 2019,
	booktitle    = {Proc. Interspeech 2019},
	pages        = {694--698},
	doi          = {10.21437/Interspeech.2019-2008}
}
@inproceedings{engel_self-supervised_2020,
	title        = {Self-supervised Pitch Detection by Inverse Audio Synthesis},
	author       = {Jesse Engel and Rigel Swavely and Lamtharn Hanoi Hantrakul and Adam Roberts and Curtis Hawthorne},
	year         = 2020,
	booktitle    = {ICML 2020 Workshop on Self-supervision in Audio and Speech},
	url          = {https://openreview.net/forum?id=RlVTYWhsky7}
}
@article{schulze-forster_unsupervised_2023,
	title        = {Unsupervised Music Source Separation Using Differentiable Parametric Source Models},
	author       = {Schulze-Forster, Kilian and Richard, Gaël and Kelley, Liam and Doire, Clement S. J. and Badeau, Roland},
	year         = 2023,
	journal      = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	volume       = 31,
	number       = {},
	pages        = {1276--1289},
	doi          = {10.1109/TASLP.2023.3252272}
}
@inproceedings{sudholt_vocal_2023,
	title        = {Vocal Tract Area Estimation by Gradient Descent},
	author       = {S{\"u}dholt, David and Cámara, Mateo and Xu, Zhiyuan and Reiss, Joshua D.},
	year         = 2023,
	booktitle    = {Proceedings of the 26th {{International Conference}} on {{Digital Audio Effects}}},
	address      = {{Copenhagen, Denmark}}
}
@inproceedings{kelly_speech_1962,
	title        = {Speech Synthesis},
	author       = {Kelly, J. L. and Lochbaum, C. C.},
	year         = 1962,
	month        = sep,
	booktitle    = {Proceedings of the Fourth International Congress on Acoustics},
	address      = {Copenhagen},
	pages        = {1--4}
}
@inproceedings{wu_mididdsp_2022,
	title        = {{MIDI}-{DDSP}: {Detailed} control of musical performance via hierarchical modeling},
	author       = {Wu, Yusong and Manilow, Ethan and Deng, Yi and Swavely, Rigel and Kastner, Kyle and Cooijmans, Tim and Courville, Aaron and Huang, Cheng-Zhi Anna and Engel, Jesse},
	year         = 2022,
	booktitle    = {International conference on learning representations},
	url          = {https://openreview.net/forum?id=UseMOjWENv}
}
@article{caillon_rave_2021,
	title        = {RAVE: A variational autoencoder for fast and high-quality neural audio synthesis},
	author       = {Caillon, Antoine and Esling, Philippe},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2111.05011}
}

